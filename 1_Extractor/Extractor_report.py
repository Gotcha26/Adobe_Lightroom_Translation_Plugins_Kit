#!/usr/bin/env python3
"""
Extractor_report.py

GÃ©nÃ©ration des rapports dÃ©taillÃ©s d'extraction et d'analyse.
"""

import os
from datetime import datetime
from typing import Dict, List
from collections import defaultdict

from Extractor_models import ExtractedString, ExtractionStats


class ReportGenerator:
    """GÃ©nÃ¨re les rapports dÃ©taillÃ©s d'extraction."""
    
    def __init__(self, plugin_path: str, prefix: str, stats: ExtractionStats):
        self.plugin_path = plugin_path
        self.prefix = prefix
        self.stats = stats
    
    def generate_report(self, extracted: List[ExtractedString], spacing_metadata: Dict[str, Dict], 
                       output_path: str):
        """GÃ©nÃ¨re le rapport dÃ©taillÃ© pour remplacement."""
        # Grouper par fichier
        by_file: Dict[str, List[ExtractedString]] = defaultdict(list)
        for entry in extracted:
            by_file[entry.file_path].append(entry)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            # En-tÃªte
            f.write("=" * 80 + "\n")
            f.write("RAPPORT D'EXTRACTION DES CHAÃNES LOCALISABLES\n")
            f.write("=" * 80 + "\n\n")
            
            f.write(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Plugin: {self.plugin_path}\n")
            f.write(f"PrÃ©fixe: {self.prefix}\n\n")
            
            # LÃ©gende des Ã©mojis
            f.write("LÃ‰GENDE:\n")
            f.write("  â¬…ï¸   = Espace(s) en DÃ‰BUT de chaÃ®ne\n")
            f.write("  â¡ï¸   = Espace(s) en FIN de chaÃ®ne\n")
            f.write("  â¬…ï¸â¡ï¸ = Espaces des DEUX cÃ´tÃ©s\n")
            f.write("  ğŸ”š  = Suffixe dÃ©tectÃ© (\" - \", \" -\", \"...\")\n")
            f.write("  ğŸ”—  = Membre d'une chaÃ®ne concatÃ©nÃ©e\n\n")
            
            # Statistiques
            f.write("STATISTIQUES\n")
            f.write("-" * 80 + "\n")
            f.write(f"Fichiers analysÃ©s          : {self.stats.files_processed}\n")
            f.write(f"Fichiers avec chaÃ®nes      : {self.stats.files_with_strings}\n")
            f.write(f"Total chaÃ®nes trouvÃ©es     : {self.stats.total_strings}\n")
            f.write(f"ClÃ©s uniques               : {self.stats.unique_strings}\n")
            f.write(f"Lignes de log ignorÃ©es     : {self.stats.log_lines_ignored}\n")
            f.write(f"ChaÃ®nes techniques ignorÃ©es: {self.stats.technical_ignored}\n")
            f.write(f"ChaÃ®nes avec espaces       : {self.stats.strings_with_spacing}\n")
            f.write(f"ChaÃ®nes avec suffixes      : {self.stats.strings_with_suffix}\n")
            f.write(f"Lignes concatÃ©nÃ©es         : {self.stats.concatenated_lines}\n")
            f.write(f"Membres de concatÃ©nation   : {self.stats.concat_members_total}\n")
            
            # Compter les clÃ©s existantes
            existing_loc_count = sum(1 for e in extracted if e.pattern_name == "existing_loc")
            f.write(f"ClÃ©s LOC existantes        : {existing_loc_count} (dÃ©jÃ  localisÃ©es, non modifiÃ©es)\n\n")
            
            # Patterns dÃ©tectÃ©s
            f.write("PATTERNS DÃ‰TECTÃ‰S\n")
            f.write("-" * 80 + "\n")
            for pattern, count in sorted(self.stats.patterns_found.items(), key=lambda x: -x[1]):
                f.write(f"  {pattern:25} : {count}\n")
            f.write("\n")
            
            # Section des clÃ©s LOC existantes (pour information)
            existing_entries = [e for e in extracted if e.pattern_name == "existing_loc"]
            if existing_entries:
                f.write("=" * 80 + "\n")
                f.write("CLÃ‰S LOC EXISTANTES (dÃ©jÃ  localisÃ©es - incluses dans PluginStrings.txt)\n")
                f.write("=" * 80 + "\n\n")
                for entry in existing_entries:
                    f.write(f"  ğŸ”’ {entry.file_path}:{entry.line_num}\n")
                    f.write(f"     ClÃ©    : {entry.suggested_key}\n")
                    f.write(f"     Valeur : {entry.base_text}\n\n")
                f.write("\n")
            
            # DÃ©tail par fichier (pour remplacement)
            f.write("=" * 80 + "\n")
            f.write("DÃ‰TAIL PAR FICHIER (pour remplacement)\n")
            f.write("=" * 80 + "\n")
            
            for file_path in sorted(by_file.keys()):
                entries = by_file[file_path]
                unique_count = len(set(e.base_text for e in entries))
                
                f.write(f"\n{'-' * 80}\n")
                f.write(f"Fichier: {file_path}\n")
                f.write(f"ChaÃ®nes: {len(entries)} ({unique_count} clÃ©s uniques)\n")
                f.write(f"{'-' * 80}\n\n")
                
                # Grouper par numÃ©ro de ligne pour afficher les concatÃ©nations ensemble
                by_line: Dict[int, List[ExtractedString]] = defaultdict(list)
                for entry in entries:
                    by_line[entry.line_num].append(entry)
                
                for line_num in sorted(by_line.keys()):
                    line_entries = by_line[line_num]
                    first_entry = line_entries[0]
                    
                    # Afficher l'en-tÃªte de la ligne
                    if first_entry.is_concat_member and len(line_entries) > 1:
                        f.write(f"  [Ligne {line_num}] Pattern: {first_entry.pattern_name} ğŸ”— CHAÃNE CONCATÃ‰NÃ‰E ({len(line_entries)} membres)\n")
                        f.write(f"  LIGNE    : {first_entry.line_content[:100]}\n")
                        
                        # Afficher chaque membre
                        for idx, entry in enumerate(line_entries, 1):
                            markers = self._get_markers(entry)
                            f.write(f"\n  MEMBRE {idx} : \"{entry.original_text}\"{markers}\n")
                            f.write(f"    BASE   : \"{entry.base_text}\"\n")
                            f.write(f"    CLÃ‰    : {entry.suggested_key}\n")
                            if entry.has_spacing():
                                f.write(f"    ESPACES: {entry.leading_spaces} dÃ©but, {entry.trailing_spaces} fin\n")
                            if entry.has_suffix():
                                f.write(f"    SUFFIXE: \"{entry.suffix}\"\n")
                        
                        f.write("\n")
                    else:
                        # ChaÃ®ne simple (non concatÃ©nÃ©e)
                        for entry in line_entries:
                            markers = self._get_markers(entry)
                            f.write(f"  [Ligne {line_num}] Pattern: {entry.pattern_name}{markers}\n")
                            f.write(f"  CHERCHER : \"{entry.original_text}\"\n")
                            f.write(f"  BASE     : \"{entry.base_text}\"\n")
                            f.write(f"  CLÃ‰      : {entry.suggested_key}\n")
                            if entry.has_spacing():
                                f.write(f"  ESPACES  : {entry.leading_spaces} dÃ©but, {entry.trailing_spaces} fin\n")
                            if entry.has_suffix():
                                f.write(f"  SUFFIXE  : \"{entry.suffix}\"\n")
                            f.write(f"  REMPLACER: {entry.replacement_code}\n\n")
            
            # ChaÃ®nes avec espaces ou suffixes (rÃ©sumÃ©)
            if spacing_metadata:
                f.write("=" * 80 + "\n")
                f.write("CHAÃNES AVEC ESPACES OU SUFFIXES\n")
                f.write("=" * 80 + "\n\n")
                f.write("Ces chaÃ®nes nÃ©cessitent une rÃ©injection des espaces/suffixes.\n\n")
                
                for i, (key, meta) in enumerate(sorted(spacing_metadata.items()), 1):
                    emojis = self._get_spacing_emojis(meta)
                    emoji_str = "".join(emojis)
                    
                    f.write(f"  {i}. {emoji_str} {key}\n")
                    f.write(f"     Original: \"{meta['original_text']}\"\n")
                    f.write(f"     Base: \"{meta.get('base_text', meta['clean_text'])}\"\n")
                    if meta['leading_spaces'] > 0 or meta['trailing_spaces'] > 0:
                        f.write(f"     Espaces: {meta['leading_spaces']} dÃ©but + {meta['trailing_spaces']} fin\n")
                    if meta.get('suffix'):
                        f.write(f"     Suffixe: \"{meta['suffix']}\"\n")
                    f.write(f"     Fichier: {meta['file']}:{meta['line']}\n\n")
            
            # Liste des clÃ©s uniques pour PluginStrings
            f.write("=" * 80 + "\n")
            f.write("LISTE DES CLÃ‰S POUR PluginStrings.txt\n")
            f.write("=" * 80 + "\n\n")
            
            # Construire la liste des clÃ©s uniques
            unique_keys: Dict[str, ExtractedString] = {}
            for entry in extracted:
                if entry.suggested_key not in unique_keys:
                    unique_keys[entry.suggested_key] = entry
            
            f.write(f"-- {len(unique_keys)} clÃ©s uniques\n\n")
            
            for entry in sorted(unique_keys.values(), key=lambda e: e.suggested_key):
                markers = self._get_markers(entry)
                # Utiliser base_text (sans suffixe) pour la valeur
                f.write(f'"{entry.suggested_key}={entry.base_text}"{markers}\n')
        
        print(f"âœ“ Rapport: {output_path}")
    
    def _get_markers(self, entry: ExtractedString) -> str:
        """Retourne la chaÃ®ne de marqueurs (Ã©mojis) pour une entrÃ©e."""
        markers = []
        if entry.spacing_emoji():
            markers.append(entry.spacing_emoji())
        if entry.suffix_emoji():
            markers.append(entry.suffix_emoji())
        if entry.concat_emoji():
            markers.append(entry.concat_emoji())
        marker_str = f" -- {''.join(markers)}" if markers else ""
        return marker_str
    
    def _get_spacing_emojis(self, meta: Dict) -> list:
        """Retourne les Ã©mojis pour les espaces et suffixes."""
        emojis = []
        if meta['leading_spaces'] > 0 and meta['trailing_spaces'] > 0:
            emojis.append("â¬…ï¸â¡ï¸")
        elif meta['leading_spaces'] > 0:
            emojis.append("â¬…ï¸")
        elif meta['trailing_spaces'] > 0:
            emojis.append("â¡ï¸")
        if meta.get('suffix'):
            emojis.append("ğŸ”š")
        return emojis
